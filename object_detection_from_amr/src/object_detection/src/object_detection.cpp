/*
 INTEL CONFIDENTIAL
 Copyright 2022 Intel Corporation
 This software and the related documents are Intel copyrighted materials, and your use of them is governed by the
 express license under which they were provided to you (License). Unless the License provides otherwise, you may not
 use, modify, copy, publish, distribute, disclose or transmit this software or the related documents without Intel's
 prior written permission.
 This software and the related documents are provided as is, with no express or implied warranties, other than those
 that are expressly stated in the License.
*/

#include <chrono>
#include <experimental/filesystem>

#include <ngraph/ngraph.hpp>

#include "object_detection.h"

using namespace std::experimental::filesystem;

ObjectDetection::ObjectDetection()
{
    bool_pc = false;
    bool_auto_resize = true;
    score_threshold = 0.75;
    maxProposalCount = 0;
    objectSize = 0;
}

ObjectDetection::~ObjectDetection()
{

}

void ObjectDetection::initInferenceEngine(const std::string &device, const std::string &model_path)
{
    std::cout << "InferenceEngine: " << GetInferenceEngineVersion() << std::endl; //inference API version, Build and Description
    ie.GetVersions(device); // just a device check
    // --------------------------- 1. Load inference engine -------------------------------------
    std::cout << "Loading Inference Engine" << std::endl;

    // --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
    std::cout << "Loading network files" << std::endl;
    /** Read network model **/
    path model_path_check = model_path;
    if (!is_regular_file(model_path_check)) {
        if (is_directory(model_path_check))
            throw std::invalid_argument("Provided path " + model_path + " should be path to model XML file!");
        else
            throw std::invalid_argument("Provided path " + model_path + " points to invalid file!");
    }
    std::cout << "Path to model: " << model_path_check.parent_path().c_str() << std::endl;
    std::cout << "Model file name: " << model_path_check.filename().c_str() << std::endl;
    std::cout << "Base file name: " <<  model_path_check.stem().c_str() << std::endl;

    /** Read labels (if any)**/
    std::string labelFilePath = model_path_check.parent_path().string() + "/" + model_path_check.stem().string() + ".labels";

    std::ifstream inputFile(labelFilePath);
    if (inputFile.is_open())
    {
        std::string label;
        while (std::getline(inputFile, label))
            labels.push_back(label);
        if (labels.empty())
            throw std::logic_error("Labels file empty: " + labelFilePath);
    }
    else
    {
        std::cout << "Could not open labels file: " << labelFilePath << std::endl;
        std::cout << "Will use a dummy label" << std::endl;
        labels.push_back("placeholder");
    }
    
    try {
        model.reset(new ModelSSD(model_path, (float)score_threshold, true, labels));
        std::string empty = "";
        pipeline = std::make_unique<AsyncPipeline>( 
            std::move(model), 
            ConfigFactory::getUserConfig(device, empty, empty, false, 0, empty, 0),
            core);
    }
    catch (const std::exception& error) {
        slog::err << error.what() << slog::endl;
    }
    catch (...) {
        slog::err << "Unknown/internal exception happened." << slog::endl;
    }

}

/*---------------------------- Helper Functions ----------------------------*/

void ObjectDetection::interpret_results(DetectionResult& result) {
    if (!result.metaData) {
        throw std::invalid_argument("Renderer: metadata is null");
    }

    auto outputImg = result.metaData->asRef<ImageMetaData>().img;

    if (outputImg.empty()) {
        throw std::invalid_argument("Renderer: image provided in metadata is empty");
        return;
    }

    det_objects.clear();
    // Visualizing result data over source image
    for (auto& obj : result.objects) {
    /*    
        slog::info << " Class ID  | Confidence | XMIN | YMIN | XMAX | YMAX " << slog::endl;
        slog::info << " "
                    << std::left << std::setw(9) << obj.label << " | "
                    << std::setw(10) << obj.confidence << " | "
                    << std::setw(4) << std::max(int(obj.x), 0) << " | "
                    << std::setw(4) << std::max(int(obj.y), 0) << " | "
                    << std::setw(4) << std::min(int(obj.x + obj.width), outputImg.cols) << " | "
                    << std::setw(4) << std::min(int(obj.y + obj.height), outputImg.rows)
                    << slog::endl;

    */    
        DetectedObjectBoxes objectdetected;
        objectdetected.class_id = (labels.size() == 1 ? 1 : (obj.labelID + 1));
        objectdetected.confidence = obj.confidence;
        objectdetected.label = (static_cast<size_t>(obj.labelID) < labels.size() ? labels[obj.labelID] : std::string("label #") + std::to_string(obj.labelID));
        objectdetected.x = std::max(int(obj.x), 0);
        objectdetected.y = std::max(int(obj.y), 0);
        objectdetected.height = obj.height;
        objectdetected.width = obj.width;
        objectdetected.score_threshold = score_threshold;
        det_objects.push_back(objectdetected);

        // TODO remove this before merge
        if  (objectdetected.class_id != 0)
           slog::info <<  "<LocalInference> Label " << labels[obj.labelID].c_str() << "\n"  << slog::endl;           
    }
}


void ObjectDetection::getBoundingBoxes(const cv::Mat &curr_frame)
{
    typedef std::chrono::duration<double, std::ratio<1, 1000>> ms;
    auto total_t0 = std::chrono::high_resolution_clock::now();
    auto wallclock = std::chrono::high_resolution_clock::now();
    double ocv_decode_time = 0, ocv_render_time = 0;

    auto t0 = std::chrono::high_resolution_clock::now();
    auto t1 = std::chrono::high_resolution_clock::now();
    ocv_decode_time = std::chrono::duration_cast<ms>(t1 - t0).count();

    t0 = std::chrono::high_resolution_clock::now();

    if (pipeline.get()->isReadyToProcess())
    {
        auto startTime = std::chrono::steady_clock::now();
        if (curr_frame.empty())
        {
            if (frameNum == -1)
            {
                throw std::logic_error("Can't read an image from the input");
            }
            else
            {
                return;
            }
        }

        frameNum = pipeline.get()->submitData(ImageInputData(curr_frame),
                    std::make_shared<ImageMetaData>(curr_frame, startTime));
    }

       
    //--- Waiting for free input slot or output data available. Function will return immediately if any of them are available.
    pipeline.get()->waitForData();

    t1 = std::chrono::high_resolution_clock::now();
    ms detection = std::chrono::duration_cast<ms>(t1 - t0);

    t0 = std::chrono::high_resolution_clock::now();
    ms wall = std::chrono::duration_cast<ms>(t0 - wallclock);
    wallclock = t0;

    t0 = std::chrono::high_resolution_clock::now();
    
    std::ostringstream out;
    out << "OpenCV cap/render time: " << std::fixed << std::setprecision(2) << (ocv_decode_time + ocv_render_time) << " ms";
    cv::putText(curr_frame, out.str(), cv::Point2f(0, 25), cv::FONT_HERSHEY_TRIPLEX, 0.6, cv::Scalar(0, 255, 0));
    out.str("");
    out << "Wallclock time (TRUE ASYNC): ";
    out << std::fixed << std::setprecision(2) << wall.count() << " ms (" << 1000.f / wall.count() << " fps)";
    cv::putText(curr_frame, out.str(), cv::Point2f(0, 50), cv::FONT_HERSHEY_TRIPLEX, 0.6, cv::Scalar(0, 0, 255));

    //--- Checking for results and rendering data if it's ready
    while (result = pipeline.get()->getResult()) 
    {
        interpret_results(result->asRef<DetectionResult>());
        framesProcessed++;
    }        

    t1 = std::chrono::high_resolution_clock::now();
    ocv_render_time = std::chrono::duration_cast<ms>(t1 - t0).count();

    auto total_t1 = std::chrono::high_resolution_clock::now();
    ms total = std::chrono::duration_cast<ms>(total_t1 - total_t0);
    slog::info << "<LocalInference> Done frame: " << framesProcessed << " . Processed in: " << total.count() << " ms \n " << slog::endl;
}
